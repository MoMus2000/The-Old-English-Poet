{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGen.ipynb",
      "provenance": [],
      "mount_file_id": "1pMewtetz34IpMM1LtK7OviN3SnnCWqbG",
      "authorship_tag": "ABX9TyO14DlR/rH6Dqz9mmhQ9Koq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoMus2000/The-Old-English-Poet/blob/master/TextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_A_mcEPE5Ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGBCtHsSzwdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "a7db8eb1-b71a-49c1-9dd6-895221a14652"
      },
      "source": [
        "!unzip /content/6776_81739_bundle_archive.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/6776_81739_bundle_archive.zip\n",
            "  inflating: Kanye_West.txt          \n",
            "  inflating: Lil_Wayne.txt           \n",
            "  inflating: adele.txt               \n",
            "  inflating: al-green.txt            \n",
            "  inflating: alicia-keys.txt         \n",
            "  inflating: amy-winehouse.txt       \n",
            "  inflating: beatles.txt             \n",
            "  inflating: bieber.txt              \n",
            "  inflating: bjork.txt               \n",
            "  inflating: blink-182.txt           \n",
            "  inflating: bob-dylan.txt           \n",
            "  inflating: bob-marley.txt          \n",
            "  inflating: britney-spears.txt      \n",
            "  inflating: bruce-springsteen.txt   \n",
            "  inflating: bruno-mars.txt          \n",
            "  inflating: cake.txt                \n",
            "  inflating: dickinson.txt           \n",
            "  inflating: disney.txt              \n",
            "  inflating: dj-khaled.txt           \n",
            "  inflating: dolly-parton.txt        \n",
            "  inflating: dr-seuss.txt            \n",
            "  inflating: drake.txt               \n",
            "  inflating: eminem.txt              \n",
            "  inflating: janisjoplin.txt         \n",
            "  inflating: jimi-hendrix.txt        \n",
            "  inflating: johnny-cash.txt         \n",
            "  inflating: joni-mitchell.txt       \n",
            "  inflating: kanye-west.txt          \n",
            "  inflating: kanye.txt               \n",
            "  inflating: lady-gaga.txt           \n",
            "  inflating: leonard-cohen.txt       \n",
            "  inflating: lil-wayne.txt           \n",
            "  inflating: lin-manuel-miranda.txt  \n",
            "  inflating: lorde.txt               \n",
            "  inflating: ludacris.txt            \n",
            "  inflating: michael-jackson.txt     \n",
            "  inflating: missy-elliott.txt       \n",
            "  inflating: nickelback.txt          \n",
            "  inflating: nicki-minaj.txt         \n",
            "  inflating: nirvana.txt             \n",
            "  inflating: notorious-big.txt       \n",
            "  inflating: notorious_big.txt       \n",
            "  inflating: nursery_rhymes.txt      \n",
            "  inflating: patti-smith.txt         \n",
            "  inflating: paul-simon.txt          \n",
            "  inflating: prince.txt              \n",
            "  inflating: r-kelly.txt             \n",
            "  inflating: radiohead.txt           \n",
            "  inflating: rihanna.txt             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYgGr-DFXqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('/content/eminem.txt').read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "corpus = [val for val in corpus if val !='']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXURmQTGb4Su",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5fe7fbf-87af-447f-8b48-967f557f74b4"
      },
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "unique_words = len(tokenizer.word_index) +1\n",
        "print(unique_words)\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequences = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequences)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl7BwcBvb6or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, padding = 'pre', maxlen= max_sequence_len))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pla-5cXub8_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]\n",
        "\n",
        "ys = to_categorical(labels, num_classes = unique_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QqW9X5afw_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bbf585d-6218-4c8f-ce0b-a420e1794726"
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print('running on tpu', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('Error .. not connected to TPU RUNTIME')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running on tpu ['10.29.238.226:8470']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkqgxR9gwee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "b91fb5d0-a352-45a3-d34a-44375579c815"
      },
      "source": [
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy=tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.29.238.226:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.29.238.226:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AavyRduYcBdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(unique_words, 200, input_length = max_sequence_len-1))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(Bidirectional(LSTM(300 , return_sequences = True)))\n",
        "  model.add(Bidirectional(LSTM(300)))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(128, activation = 'relu'))\n",
        "  model.add(Dense(64, activation = 'relu'))\n",
        "  model.add(Dense(unique_words,activation = 'softmax'))\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr = 0.0005))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35tsTFyuhB7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model = model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3HkAKShaZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd6ca07d-8461-4e6b-b1b5-0fbb12a54445"
      },
      "source": [
        "history = model.fit(xs,ys, batch_size=256, epochs = 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "227/227 [==============================] - 32s 142ms/step - loss: 6.8509\n",
            "Epoch 2/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 6.4443\n",
            "Epoch 3/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 6.3706\n",
            "Epoch 4/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 6.2519\n",
            "Epoch 5/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 6.0906\n",
            "Epoch 6/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.9110\n",
            "Epoch 7/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.7627\n",
            "Epoch 8/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.6348\n",
            "Epoch 9/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.5043\n",
            "Epoch 10/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.3839\n",
            "Epoch 11/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.2734\n",
            "Epoch 12/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 5.1709\n",
            "Epoch 13/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 5.0702\n",
            "Epoch 14/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.9721\n",
            "Epoch 15/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.8844\n",
            "Epoch 16/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 4.8010\n",
            "Epoch 17/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.7115\n",
            "Epoch 18/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.6345\n",
            "Epoch 19/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 4.5560\n",
            "Epoch 20/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.4755\n",
            "Epoch 21/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.4022\n",
            "Epoch 22/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 4.3300\n",
            "Epoch 23/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.2574\n",
            "Epoch 24/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.1947\n",
            "Epoch 25/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 4.1227\n",
            "Epoch 26/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 4.0642\n",
            "Epoch 27/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.9943\n",
            "Epoch 28/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.9421\n",
            "Epoch 29/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.8711\n",
            "Epoch 30/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.8176\n",
            "Epoch 31/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.7744\n",
            "Epoch 32/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.7073\n",
            "Epoch 33/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.6636\n",
            "Epoch 34/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.6044\n",
            "Epoch 35/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.5675\n",
            "Epoch 36/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.5166\n",
            "Epoch 37/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.4638\n",
            "Epoch 38/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.4082\n",
            "Epoch 39/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.3689\n",
            "Epoch 40/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.3238\n",
            "Epoch 41/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.2868\n",
            "Epoch 42/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.2384\n",
            "Epoch 43/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 3.1968\n",
            "Epoch 44/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.1639\n",
            "Epoch 45/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.1248\n",
            "Epoch 46/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 3.0823\n",
            "Epoch 47/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 3.0462\n",
            "Epoch 48/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 3.0060\n",
            "Epoch 49/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.9673\n",
            "Epoch 50/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.9431\n",
            "Epoch 51/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.8962\n",
            "Epoch 52/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.8678\n",
            "Epoch 53/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.8407\n",
            "Epoch 54/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.8099\n",
            "Epoch 55/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.7927\n",
            "Epoch 56/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.7421\n",
            "Epoch 57/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.6965\n",
            "Epoch 58/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.6724\n",
            "Epoch 59/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.6592\n",
            "Epoch 60/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.6286\n",
            "Epoch 61/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.5963\n",
            "Epoch 62/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.5623\n",
            "Epoch 63/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.5248\n",
            "Epoch 64/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.4989\n",
            "Epoch 65/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.4675\n",
            "Epoch 66/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.4364\n",
            "Epoch 67/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.4275\n",
            "Epoch 68/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.4043\n",
            "Epoch 69/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.3710\n",
            "Epoch 70/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 2.3537\n",
            "Epoch 71/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.3302\n",
            "Epoch 72/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.2999\n",
            "Epoch 73/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 2.2674\n",
            "Epoch 74/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.2493\n",
            "Epoch 75/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.2273\n",
            "Epoch 76/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.2144\n",
            "Epoch 77/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.1663\n",
            "Epoch 78/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.1555\n",
            "Epoch 79/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.1281\n",
            "Epoch 80/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 2.1011\n",
            "Epoch 81/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.0872\n",
            "Epoch 82/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.0745\n",
            "Epoch 83/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.0492\n",
            "Epoch 84/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.0169\n",
            "Epoch 85/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 2.0084\n",
            "Epoch 86/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.9830\n",
            "Epoch 87/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.9633\n",
            "Epoch 88/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.9629\n",
            "Epoch 89/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.9297\n",
            "Epoch 90/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.9185\n",
            "Epoch 91/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.8998\n",
            "Epoch 92/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.8944\n",
            "Epoch 93/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.8613\n",
            "Epoch 94/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.8411\n",
            "Epoch 95/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 1.8256\n",
            "Epoch 96/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.7976\n",
            "Epoch 97/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.8031\n",
            "Epoch 98/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.7777\n",
            "Epoch 99/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.7778\n",
            "Epoch 100/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.7479\n",
            "Epoch 101/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.7773\n",
            "Epoch 102/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.7622\n",
            "Epoch 103/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.7094\n",
            "Epoch 104/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.6929\n",
            "Epoch 105/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.6655\n",
            "Epoch 106/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 1.6700\n",
            "Epoch 107/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.6475\n",
            "Epoch 108/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.6340\n",
            "Epoch 109/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.6069\n",
            "Epoch 110/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.6069\n",
            "Epoch 111/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.5925\n",
            "Epoch 112/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 1.5780\n",
            "Epoch 113/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.5663\n",
            "Epoch 114/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.5816\n",
            "Epoch 115/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.5427\n",
            "Epoch 116/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.5269\n",
            "Epoch 117/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.5100\n",
            "Epoch 118/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.5367\n",
            "Epoch 119/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.5203\n",
            "Epoch 120/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.4976\n",
            "Epoch 121/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.4687\n",
            "Epoch 122/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.4397\n",
            "Epoch 123/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.4406\n",
            "Epoch 124/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.4428\n",
            "Epoch 125/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.4387\n",
            "Epoch 126/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.4112\n",
            "Epoch 127/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.3872\n",
            "Epoch 128/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.4078\n",
            "Epoch 129/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.4006\n",
            "Epoch 130/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3976\n",
            "Epoch 131/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3861\n",
            "Epoch 132/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.3507\n",
            "Epoch 133/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3377\n",
            "Epoch 134/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3499\n",
            "Epoch 135/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3371\n",
            "Epoch 136/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3249\n",
            "Epoch 137/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 1.3185\n",
            "Epoch 138/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3328\n",
            "Epoch 139/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.3181\n",
            "Epoch 140/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.2980\n",
            "Epoch 141/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2825\n",
            "Epoch 142/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.3195\n",
            "Epoch 143/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 1.2710\n",
            "Epoch 144/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2743\n",
            "Epoch 145/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2506\n",
            "Epoch 146/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2474\n",
            "Epoch 147/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2312\n",
            "Epoch 148/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.2399\n",
            "Epoch 149/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.2284\n",
            "Epoch 150/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.2348\n",
            "Epoch 151/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1973\n",
            "Epoch 152/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1887\n",
            "Epoch 153/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1768\n",
            "Epoch 154/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1575\n",
            "Epoch 155/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1760\n",
            "Epoch 156/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1781\n",
            "Epoch 157/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1488\n",
            "Epoch 158/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1405\n",
            "Epoch 159/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1622\n",
            "Epoch 160/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1468\n",
            "Epoch 161/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1357\n",
            "Epoch 162/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1423\n",
            "Epoch 163/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1576\n",
            "Epoch 164/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1432\n",
            "Epoch 165/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1088\n",
            "Epoch 166/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 1.1177\n",
            "Epoch 167/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 1.1308\n",
            "Epoch 168/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.1093\n",
            "Epoch 169/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0933\n",
            "Epoch 170/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0722\n",
            "Epoch 171/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0944\n",
            "Epoch 172/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1062\n",
            "Epoch 173/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0787\n",
            "Epoch 174/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0641\n",
            "Epoch 175/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0848\n",
            "Epoch 176/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0519\n",
            "Epoch 177/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0645\n",
            "Epoch 178/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 1.1154\n",
            "Epoch 179/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 1.0616\n",
            "Epoch 180/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.0390\n",
            "Epoch 181/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0167\n",
            "Epoch 182/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0182\n",
            "Epoch 183/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9974\n",
            "Epoch 184/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9959\n",
            "Epoch 185/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.0159\n",
            "Epoch 186/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.0368\n",
            "Epoch 187/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0266\n",
            "Epoch 188/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 1.0240\n",
            "Epoch 189/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.0167\n",
            "Epoch 190/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 1.1095\n",
            "Epoch 191/400\n",
            "227/227 [==============================] - 26s 112ms/step - loss: 1.0308\n",
            "Epoch 192/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9850\n",
            "Epoch 193/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9567\n",
            "Epoch 194/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9698\n",
            "Epoch 195/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9433\n",
            "Epoch 196/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9579\n",
            "Epoch 197/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9682\n",
            "Epoch 198/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9579\n",
            "Epoch 199/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9716\n",
            "Epoch 200/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9798\n",
            "Epoch 201/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9723\n",
            "Epoch 202/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9695\n",
            "Epoch 203/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9352\n",
            "Epoch 204/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9472\n",
            "Epoch 205/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9387\n",
            "Epoch 206/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9522\n",
            "Epoch 207/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9385\n",
            "Epoch 208/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9301\n",
            "Epoch 209/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9256\n",
            "Epoch 210/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 0.9132\n",
            "Epoch 211/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9605\n",
            "Epoch 212/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9805\n",
            "Epoch 213/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9412\n",
            "Epoch 214/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9034\n",
            "Epoch 215/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8954\n",
            "Epoch 216/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.9416\n",
            "Epoch 217/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.9389\n",
            "Epoch 218/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8878\n",
            "Epoch 219/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8883\n",
            "Epoch 220/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8892\n",
            "Epoch 221/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8708\n",
            "Epoch 222/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8935\n",
            "Epoch 223/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.9054\n",
            "Epoch 224/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8834\n",
            "Epoch 225/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8722\n",
            "Epoch 226/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8967\n",
            "Epoch 227/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8952\n",
            "Epoch 228/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.8882\n",
            "Epoch 229/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8870\n",
            "Epoch 230/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8997\n",
            "Epoch 231/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8601\n",
            "Epoch 232/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8516\n",
            "Epoch 233/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 0.8633\n",
            "Epoch 234/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8562\n",
            "Epoch 235/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8551\n",
            "Epoch 236/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8679\n",
            "Epoch 237/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8643\n",
            "Epoch 238/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8427\n",
            "Epoch 239/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8399\n",
            "Epoch 240/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.8471\n",
            "Epoch 241/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.8746\n",
            "Epoch 242/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8639\n",
            "Epoch 243/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8802\n",
            "Epoch 244/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8219\n",
            "Epoch 245/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8442\n",
            "Epoch 246/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8218\n",
            "Epoch 247/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8162\n",
            "Epoch 248/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8126\n",
            "Epoch 249/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8111\n",
            "Epoch 250/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.8284\n",
            "Epoch 251/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8273\n",
            "Epoch 252/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8163\n",
            "Epoch 253/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8120\n",
            "Epoch 254/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8109\n",
            "Epoch 255/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8133\n",
            "Epoch 256/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8208\n",
            "Epoch 257/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8056\n",
            "Epoch 258/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8101\n",
            "Epoch 259/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8045\n",
            "Epoch 260/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.8063\n",
            "Epoch 261/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8007\n",
            "Epoch 262/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8076\n",
            "Epoch 263/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8084\n",
            "Epoch 264/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8113\n",
            "Epoch 265/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.8186\n",
            "Epoch 266/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8115\n",
            "Epoch 267/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7956\n",
            "Epoch 268/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7768\n",
            "Epoch 269/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7648\n",
            "Epoch 270/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7746\n",
            "Epoch 271/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7710\n",
            "Epoch 272/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7939\n",
            "Epoch 273/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7725\n",
            "Epoch 274/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7938\n",
            "Epoch 275/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7987\n",
            "Epoch 276/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7931\n",
            "Epoch 277/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7688\n",
            "Epoch 278/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.7646\n",
            "Epoch 279/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7644\n",
            "Epoch 280/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7884\n",
            "Epoch 281/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.8064\n",
            "Epoch 282/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7886\n",
            "Epoch 283/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7605\n",
            "Epoch 284/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7479\n",
            "Epoch 285/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7387\n",
            "Epoch 286/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7392\n",
            "Epoch 287/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.7505\n",
            "Epoch 288/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.7452\n",
            "Epoch 289/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7832\n",
            "Epoch 290/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.7687\n",
            "Epoch 291/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7458\n",
            "Epoch 292/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7418\n",
            "Epoch 293/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7229\n",
            "Epoch 294/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7378\n",
            "Epoch 295/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7590\n",
            "Epoch 296/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7415\n",
            "Epoch 297/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7425\n",
            "Epoch 298/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7580\n",
            "Epoch 299/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7365\n",
            "Epoch 300/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7507\n",
            "Epoch 301/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7620\n",
            "Epoch 302/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7556\n",
            "Epoch 303/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7341\n",
            "Epoch 304/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7430\n",
            "Epoch 305/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7398\n",
            "Epoch 306/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7336\n",
            "Epoch 307/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7246\n",
            "Epoch 308/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7011\n",
            "Epoch 309/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7029\n",
            "Epoch 310/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7176\n",
            "Epoch 311/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7004\n",
            "Epoch 312/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7031\n",
            "Epoch 313/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7222\n",
            "Epoch 314/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7337\n",
            "Epoch 315/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7131\n",
            "Epoch 316/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7746\n",
            "Epoch 317/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7698\n",
            "Epoch 318/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7123\n",
            "Epoch 319/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7059\n",
            "Epoch 320/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7024\n",
            "Epoch 321/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6993\n",
            "Epoch 322/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7018\n",
            "Epoch 323/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7294\n",
            "Epoch 324/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7032\n",
            "Epoch 325/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6854\n",
            "Epoch 326/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6971\n",
            "Epoch 327/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.7035\n",
            "Epoch 328/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7100\n",
            "Epoch 329/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7089\n",
            "Epoch 330/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7066\n",
            "Epoch 331/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7121\n",
            "Epoch 332/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7502\n",
            "Epoch 333/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7353\n",
            "Epoch 334/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6941\n",
            "Epoch 335/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6898\n",
            "Epoch 336/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.7040\n",
            "Epoch 337/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7026\n",
            "Epoch 338/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6974\n",
            "Epoch 339/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7229\n",
            "Epoch 340/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7245\n",
            "Epoch 341/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7162\n",
            "Epoch 342/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6790\n",
            "Epoch 343/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6809\n",
            "Epoch 344/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6745\n",
            "Epoch 345/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.6957\n",
            "Epoch 346/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6876\n",
            "Epoch 347/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6912\n",
            "Epoch 348/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6755\n",
            "Epoch 349/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6818\n",
            "Epoch 350/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.7039\n",
            "Epoch 351/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7152\n",
            "Epoch 352/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7277\n",
            "Epoch 353/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6914\n",
            "Epoch 354/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6634\n",
            "Epoch 355/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6687\n",
            "Epoch 356/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6792\n",
            "Epoch 357/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6779\n",
            "Epoch 358/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6776\n",
            "Epoch 359/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6658\n",
            "Epoch 360/400\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.6727\n",
            "Epoch 361/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7301\n",
            "Epoch 362/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.7449\n",
            "Epoch 363/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.6941\n",
            "Epoch 364/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.6818\n",
            "Epoch 365/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6637\n",
            "Epoch 366/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6513\n",
            "Epoch 367/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6498\n",
            "Epoch 368/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6707\n",
            "Epoch 369/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6789\n",
            "Epoch 370/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6764\n",
            "Epoch 371/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6588\n",
            "Epoch 372/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6635\n",
            "Epoch 373/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6694\n",
            "Epoch 374/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6818\n",
            "Epoch 375/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6839\n",
            "Epoch 376/400\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.6827\n",
            "Epoch 377/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6611\n",
            "Epoch 378/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6580\n",
            "Epoch 379/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6522\n",
            "Epoch 380/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6565\n",
            "Epoch 381/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6488\n",
            "Epoch 382/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6541\n",
            "Epoch 383/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6711\n",
            "Epoch 384/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6478\n",
            "Epoch 385/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6380\n",
            "Epoch 386/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6497\n",
            "Epoch 387/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6600\n",
            "Epoch 388/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6610\n",
            "Epoch 389/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6792\n",
            "Epoch 390/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6685\n",
            "Epoch 391/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6738\n",
            "Epoch 392/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6679\n",
            "Epoch 393/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6495\n",
            "Epoch 394/400\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6533\n",
            "Epoch 395/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6404\n",
            "Epoch 396/400\n",
            "227/227 [==============================] - 25s 109ms/step - loss: 0.6277\n",
            "Epoch 397/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6305\n",
            "Epoch 398/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6376\n",
            "Epoch 399/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6522\n",
            "Epoch 400/400\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.6505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd6nsn2XutK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "83e21299-ccdb-454a-aabd-57f070318299"
      },
      "source": [
        "seed_text = \"We'll walk this road together, through the storm\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif (index) == predicted[0]:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text, sep = \"i\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We'll walk this road together, through the storm and chillin' in a trailer of is empty bitch that i can touch 'em making a one time 'cause i'm tired of 'em 'till tomorrow you spittin' you moves you was fuck 'em when i'm fuck it they must stop rage to suc i get fuckin' rhyme in the pop tracks think i can conversation stars i'm glad i made it 'til you go in three bitch in the pop reason i can murdered the genes with me there there's any side i'm depressed for fuckin' track to hide back it there sometimes shit we hits you know we been\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eqaYXC8dnrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4111d783-a3e7-4345-8fbc-18136fe224ef"
      },
      "source": [
        "import "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement textwrap (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for textwrap\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}